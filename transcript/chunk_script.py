#!/usr/bin/env python3
"""
Semantic Chunking Script (Modified for Sentence-Based Chunking)

This script performs semantic chunking on ASR transcripts generated by the asr_script.py.
It creates optimal chunks based on sentence boundaries, respecting a maximum size limit.

Requirements:
    - Python 3.8+
    - pandas
    - nltk
    - spacy (and model like en_core_web_trf)
    - tqdm
    - uuid

Usage:
    python chunk_script.py --input transcript.json --output chunks.csv --max_tokens 200
"""

import argparse
import json
import os
import uuid
from datetime import datetime
import pandas as pd
import nltk
import spacy
from tqdm import tqdm
import re

# Download NLTK resources (needed for word_tokenize used in timestamp mapping)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    # nltk.download('punkt_tab') # punkt_tab seems deprecated or specific, using just punkt
    nltk.download('punkt')

class TranscriptChunker:
    def __init__(self, max_tokens=200, overlap_tokens=20, min_chunk_tokens=10): # Reduced min_chunk_tokens default
        """
        Initialize the chunker with parameters.

        Args:
            max_tokens: Maximum number of tokens per chunk
            overlap_tokens: Number of overlapping tokens between chunks (applied at sentence level if possible)
            min_chunk_tokens: Minimum number of tokens for a valid chunk (used for filtering, not merging)
        """
        if max_tokens <= overlap_tokens:
            raise ValueError("max_tokens must be greater than overlap_tokens")
        if min_chunk_tokens >= max_tokens:
             raise ValueError("min_chunk_tokens should be significantly smaller than max_tokens")

        self.max_tokens = max_tokens
        self.overlap_tokens = overlap_tokens
        # We will use min_chunk_tokens to potentially filter, not merge.
        self.min_chunk_tokens = min_chunk_tokens

        # Attempt to use GPU with spaCy and load a GPU-accelerated transformer model.
        print("Attempting to enable GPU for spaCy processing...")
        try:
            spacy.require_gpu()
            print("GPU enabled for spaCy.")
        except Exception as e:
            print("GPU not available or spaCy configuration issue, using CPU for spaCy.")

        print("Loading spaCy model (en_core_web_trf recommended)...")
        # Ensure "en_core_web_trf" is installed: python -m spacy download en_core_web_trf
        try:
            self.nlp = spacy.load("en_core_web_trf")
        except OSError:
            print("Warning: 'en_core_web_trf' not found. Falling back to 'en_core_web_sm'.")
            print("For better performance, install the transformer model: python -m spacy download en_core_web_trf")
            try:
                 self.nlp = spacy.load("en_core_web_sm")
            except OSError:
                 print("Error: No spaCy English model found (tried en_core_web_trf, en_core_web_sm).")
                 print("Please install a model, e.g., python -m spacy download en_core_web_sm")
                 raise # Re-raise the error to stop execution

        # NLTK word tokenizer is still needed for mapping spaCy spans back to original word indices for timestamps
        self._nltk_word_tokenize = nltk.word_tokenize

    def _get_token_indices_for_span(self, text, span_start_char, span_end_char, all_tokens_info):
        """Maps character spans (from spaCy) to NLTK token indices."""
        start_token_idx = -1
        end_token_idx = -1

        for i, token_info in enumerate(all_tokens_info):
            token_start, token_end = token_info['span']
            # Find the first token that starts at or after the span start
            if start_token_idx == -1 and token_start >= span_start_char:
                 start_token_idx = i
            # Find the last token that ends at or before the span end
            if token_end <= span_end_char:
                 end_token_idx = i
            # Optimization: If we've passed the span end, we can stop
            elif token_start >= span_end_char:
                 break

        # Handle cases where span might not align perfectly with token boundaries
        if start_token_idx == -1 and len(all_tokens_info) > 0:
            start_token_idx = 0 # Default to first token if span starts before it
        if end_token_idx == -1 and start_token_idx != -1:
             # If no token ends *before* the span end, take the first token within the span
             end_token_idx = start_token_idx
        elif start_token_idx > end_token_idx and start_token_idx != -1 :
             # If start is after end (e.g. span is between tokens), adjust
             end_token_idx = start_token_idx

        # Ensure end_token_idx is not before start_token_idx
        end_token_idx = max(start_token_idx, end_token_idx)

        # Handle possibility that no tokens were found in range (e.g. empty span)
        if start_token_idx == -1:
            return -1, -1

        return start_token_idx, end_token_idx


    def get_timestamp_for_token_position(self, transcript_data, token_index, segment_search_start_index=0):
        """
        Find the timestamp for a specific NLTK word token index in the transcript.
        Uses the word-level timestamp info from ASR.

        Args:
            transcript_data: Transcript data from ASR script
            token_index: Index of the NLTK word token in the full transcript text
            segment_search_start_index: Optional starting segment index for optimization

        Returns:
            Tuple of (timestamp, segment_id) or (None, None) if not found
        """
        segments = transcript_data["transcript"].get("segments", [])
        if not segments:
            return None, None

        current_word_count = 0
        # Use word counts from ASR data, not NLTK token counts directly
        words_in_segments = [(i, seg.get("words", [])) for i, seg in enumerate(segments)]

        # Flatten the word list while keeping track of cumulative count and segment index
        cumulative_word_index = 0
        for seg_idx in range(segment_search_start_index, len(segments)):
            segment = segments[seg_idx]
            segment_words = segment.get("words", [])
            num_words_in_segment = len(segment_words)

            if cumulative_word_index + num_words_in_segment > token_index:
                # The target token falls within this segment
                relative_word_index = token_index - cumulative_word_index
                if 0 <= relative_word_index < num_words_in_segment:
                    word_info = segment_words[relative_word_index]
                    # Use 'start' for start time, 'end' for end time lookup if needed later
                    return word_info.get("start"), segment["id"]
                else:
                    # Should ideally not happen if token_index is valid, but fallback to segment boundary
                    # Use the start of the first word or segment start if no words
                    # Use the end of the last word or segment end if no words
                    # For simplicity in lookup, let's return segment start/end based on relative index
                    if relative_word_index < 0 and segment_words:
                         return segment_words[0].get("start"), segment["id"]
                    elif segment_words:
                         return segment_words[-1].get("end"), segment["id"]
                    else: # Segment has no words
                         return segment.get("start"), segment["id"] # Or end? Start seems safer for start_time

            cumulative_word_index += num_words_in_segment

        # If token_index is beyond the last word, return the end time of the last segment
        if segments:
             last_segment = segments[-1]
             last_segment_words = last_segment.get("words", [])
             if last_segment_words:
                  return last_segment_words[-1].get("end"), last_segment["id"]
             else:
                  return last_segment.get("end"), last_segment["id"]

        return None, None # Should not be reached if transcript_data is valid

    def chunk_by_sentences(self, transcript_data):
        """
        Chunk the transcript by sentences using spaCy, respecting max_tokens.

        Args:
            transcript_data: Transcript data from ASR script

        Returns:
            List of chunk dictionaries with text and timestamp info
        """
        full_text = transcript_data["transcript"]["text"]
        if not full_text.strip():
            print("Warning: Transcript text is empty.")
            return []

        # 1. Tokenize full text with NLTK AND get character spans for mapping
        #    We need NLTK tokens because get_timestamp_for_token_position uses word counts from ASR,
        #    which often aligns better with simple whitespace/punctuation splitting like NLTK's.
        print("Tokenizing text with NLTK...")
        nltk_tokens = []
        all_tokens_info = []
        current_pos = 0
        # Use NLTK's span_tokenize for character offsets
        for start, end in nltk.tokenize.WhitespaceTokenizer().span_tokenize(full_text):
             token_text = full_text[start:end]
             # Further split based on punctuation if needed (mimics word_tokenize more closely)
             # Simple approach: just use whitespace tokens for index mapping. ASR word counts are key.
             # If ASR word counts differ significantly from NLTK token count, timestamp mapping might drift.
             nltk_tokens.append(token_text)
             all_tokens_info.append({'text': token_text, 'span': (start, end)})
             # Note: word_tokenize might split "don't" into "do", "n't". WhitespaceTokenizer keeps it one token.
             # This difference *could* affect timestamp mapping if ASR split "don't". Assuming ASR uses word boundaries similar to simple splitting.

        if not all_tokens_info:
            print("Warning: NLTK tokenization resulted in zero tokens.")
            return []

        # 2. Process text with spaCy to get sentences
        print("Processing text with spaCy for sentence boundaries...")
        doc = self.nlp(full_text)
        sentences = list(doc.sents)

        chunks = []
        current_chunk_sentences = []
        current_chunk_token_count = 0
        start_sentence_index = 0
        last_used_segment_idx = 0 # For optimizing timestamp search

        # Variables to manage overlap - store indices of the *last* sentence added
        last_added_sent_start_token_idx = 0
        last_added_sent_end_token_idx = 0

        print(f"Grouping {len(sentences)} sentences into chunks (max_tokens={self.max_tokens})...")
        for i, sent in enumerate(tqdm(sentences, desc="Chunking Sentences")):
            sent_text = sent.text.strip()
            if not sent_text:
                continue

            # Estimate token count for the current sentence more accurately using NLTK tokenizer on sentence text
            # This aligns better with max_tokens logic which is often based on word counts.
            sent_token_count = len(self._nltk_word_tokenize(sent_text))
            # Find NLTK token indices for this sentence span to get timestamps later
            # sent_start_char = sent.start_char
            # sent_end_char = sent.end_char
            # sent_start_token_idx, sent_end_token_idx = self._get_token_indices_for_span(
            #     full_text, sent_start_char, sent_end_char, all_tokens_info
            # )
            # Map sentence char offsets to NLTK token indices
            sent_start_token_idx, sent_end_token_idx = self._get_token_indices_for_span(
                full_text, sent.start_char, sent.end_char, all_tokens_info)


            if sent_start_token_idx == -1: # Skip if sentence couldn't be mapped
                 print(f"Warning: Could not map sentence {i} to tokens: '{sent_text}'")
                 continue

            # Check if adding this sentence would exceed the max token limit
            if current_chunk_token_count > 0 and current_chunk_token_count + sent_token_count > self.max_tokens:
                # Finalize the current chunk
                chunk_text = " ".join(s.text.strip() for s in current_chunk_sentences)

                # Get start token index from the first sentence in the chunk
                first_sent = current_chunk_sentences[0]
                chunk_start_token_idx, _ = self._get_token_indices_for_span(
                     full_text, first_sent.start_char, first_sent.end_char, all_tokens_info)

                # Get end token index from the last sentence in the chunk
                last_sent = current_chunk_sentences[-1]
                _, chunk_end_token_idx = self._get_token_indices_for_span(
                     full_text, last_sent.start_char, last_sent.end_char, all_tokens_info)

                # Ensure indices are valid before timestamp lookup
                if chunk_start_token_idx != -1 and chunk_end_token_idx != -1:
                    start_time, start_segment_id = self.get_timestamp_for_token_position(
                        transcript_data, chunk_start_token_idx, last_used_segment_idx)
                    # Use end_token_idx for end time. The function handles index correctly.
                    end_time, end_segment_id = self.get_timestamp_for_token_position(
                        transcript_data, chunk_end_token_idx, start_segment_id or 0) # Start search from start_segment_id

                    if start_time is not None and end_time is not None:
                         chunk_id = str(uuid.uuid4())
                         chunk_data = {
                             "clip_id": chunk_id,
                             "audio_id": transcript_data["audio_id"],
                             "start_time": start_time,
                             "end_time": end_time,
                             "transcript": chunk_text,
                             "created_at": datetime.now().isoformat(),
                             "updated_at": datetime.now().isoformat(),
                         }
                         chunks.append(chunk_data)
                         last_used_segment_idx = start_segment_id or 0 # Update for next search optimization
                    else:
                         print(f"Warning: Could not get valid timestamps for chunk ending with sentence {i-1}. Skipping chunk.")

                # Start new chunk: Implement overlap by backtracking tokens
                current_chunk_sentences = []
                current_chunk_token_count = 0

                # Overlap Logic: Try to find a sentence boundary near `overlap_tokens` back from the end of the *previous* chunk.
                overlap_start_token_idx = max(0, chunk_end_token_idx - self.overlap_tokens)
                # Find the sentence that contains this overlap_start_token_idx
                overlap_sentence_found = False
                for j in range(len(current_chunk_sentences) -1, -1, -1): # Check previous chunk's sentences in reverse
                     prev_sent = current_chunk_sentences[j] # Actually, should check sentences *before* the current `sent`
                     # Need to check sentences from the *previous* chunk... this requires rethinking the loop structure or storing more state.

                # Simpler overlap: Just restart from the current sentence that caused the break.
                # This creates overlap implicitly if sentences are long.
                # A better way: backtrack `overlap_tokens` from `chunk_end_token_idx` and find the sentence starting *before* or *at* that point.
                # Let's stick to a simpler method first: restart with the current sentence.
                # To implement token-based overlap cleanly with sentence chunking:
                # We need the start_token_idx of the *next* chunk. It should be `chunk_end_token_idx - overlap_tokens + 1`.
                # Find which sentence this index falls into, and start the next chunk from *that* sentence.

                next_chunk_start_token_idx = max(0, chunk_end_token_idx - self.overlap_tokens + 1)
                # Find the sentence index `j` such that sentence `j` contains `next_chunk_start_token_idx`.
                start_sentence_index = i # Default: start with the current sentence
                for sent_idx_search in range(i - 1, -1, -1):
                     search_sent = sentences[sent_idx_search]
                     s_start, s_end = self._get_token_indices_for_span(full_text, search_sent.start_char, search_sent.end_char, all_tokens_info)
                     if s_start != -1 and s_start <= next_chunk_start_token_idx:
                          start_sentence_index = sent_idx_search
                          # print(f"Overlap: Starting next chunk from sentence {start_sentence_index} (token index {s_start}) to cover overlap from token {next_chunk_start_token_idx}")
                          break

                # Rebuild the start of the new chunk based on the determined start_sentence_index
                for j in range(start_sentence_index, i + 1): # Include the current sentence `i`
                     sent_to_add = sentences[j]
                     sent_text_to_add = sent_to_add.text.strip()
                     if not sent_text_to_add: continue
                     current_chunk_sentences.append(sent_to_add)
                     current_chunk_token_count += len(self._nltk_word_tokenize(sent_text_to_add))
                     # Update last added sentence indices for the *next* potential overlap calculation
                     # last_added_sent_start_token_idx, last_added_sent_end_token_idx = self._get_token_indices_for_span(
                     #     full_text, sent_to_add.start_char, sent_to_add.end_char, all_tokens_info
                     # )

            else:
                # Add current sentence to the chunk
                current_chunk_sentences.append(sent)
                current_chunk_token_count += sent_token_count
                # last_added_sent_start_token_idx = sent_start_token_idx
                # last_added_sent_end_token_idx = sent_end_token_idx


        # Add the last remaining chunk
        if current_chunk_sentences:
            chunk_text = " ".join(s.text.strip() for s in current_chunk_sentences)
            # Get start token index from the first sentence
            first_sent = current_chunk_sentences[0]
            chunk_start_token_idx, _ = self._get_token_indices_for_span(
                 full_text, first_sent.start_char, first_sent.end_char, all_tokens_info)

            # Get end token index from the last sentence
            last_sent = current_chunk_sentences[-1]
            _, chunk_end_token_idx = self._get_token_indices_for_span(
                 full_text, last_sent.start_char, last_sent.end_char, all_tokens_info)

            # Ensure indices are valid
            if chunk_start_token_idx != -1 and chunk_end_token_idx != -1:
                start_time, start_segment_id = self.get_timestamp_for_token_position(
                    transcript_data, chunk_start_token_idx, last_used_segment_idx)
                end_time, end_segment_id = self.get_timestamp_for_token_position(
                    transcript_data, chunk_end_token_idx, start_segment_id or 0)

                if start_time is not None and end_time is not None:
                    chunk_id = str(uuid.uuid4())
                    chunk_data = {
                        "clip_id": chunk_id,
                        "audio_id": transcript_data["audio_id"],
                        "start_time": start_time,
                        "end_time": end_time,
                        "transcript": chunk_text,
                        "created_at": datetime.now().isoformat(),
                        "updated_at": datetime.now().isoformat(),
                    }
                    chunks.append(chunk_data)
                else:
                    print(f"Warning: Could not get valid timestamps for the final chunk. Skipping.")


        print(f"Sentence-based chunking created {len(chunks)} chunks.")
        return chunks


    def analyze_chunks_semantic_quality(self, chunks):
        """
        Analyze the semantic quality of chunks using spaCy.
        (Kept the same basic logic, but applied to potentially shorter chunks)

        Args:
            chunks: List of chunk dictionaries

        Returns:
            List of chunks with confidence scores
        """
        if not chunks:
             return []

        print("Analyzing semantic quality of chunks...")
        for chunk in tqdm(chunks, desc="Analyzing Semantic Quality"):
            # Process with spaCy for semantic analysis
            # Check if transcript is valid string and not empty
            transcript_text = chunk.get("transcript", "")
            if not isinstance(transcript_text, str) or not transcript_text.strip():
                chunk["confidence_score"] = 0.1 # Very low confidence for empty/invalid
                continue

            doc = self.nlp(transcript_text)

            # Calculate a basic coherence score (0-1)
            sentences = list(doc.sents)
            if not sentences:
                coherence = 0.5  # Neutral if no sentences found by spaCy
            else:
                # Check if first significant char is uppercase and last significant non-whitespace ends with punctuation
                first_sent_text = sentences[0].text.strip()
                last_sent_text = sentences[-1].text.strip()

                first_complete = bool(re.match(r'^[A-Z]', first_sent_text)) if first_sent_text else False
                last_complete = bool(re.search(r'[.!?]$', last_sent_text)) if last_sent_text else False

                # Give higher score if both ends seem complete
                if first_complete and last_complete:
                    coherence = 0.95
                elif first_complete or last_complete:
                    coherence = 0.75
                else:
                    coherence = 0.5 # Seems incomplete at both ends

            # Set confidence score, clamped between 0.5 and 0.95
            chunk["confidence_score"] = min(0.95, max(0.5, coherence))

        return chunks

    def filter_short_chunks(self, chunks):
        """
        Filters out chunks that are below the minimum token count.
        Replaces the old `optimize_chunks` which merged short chunks.

        Args:
            chunks: List of chunk dictionaries

        Returns:
            Filtered list of chunks
        """
        if not chunks:
             return []

        print(f"Filtering chunks shorter than {self.min_chunk_tokens} tokens...")
        filtered_chunks = []
        for chunk in chunks:
             # Estimate token count using the same method as chunking
             token_count = len(self._nltk_word_tokenize(chunk.get("transcript", "")))
             if token_count >= self.min_chunk_tokens:
                  filtered_chunks.append(chunk)
             else:
                  # Log the filtered chunk for debugging if needed
                  # print(f"Filtering out short chunk (ID: {chunk['clip_id']}, Tokens: {token_count}): '{chunk['transcript'][:50]}...'")
                  pass # Removed print for cleaner output

        print(f"Retained {len(filtered_chunks)} chunks after filtering.")
        return filtered_chunks


    def process_transcript(self, transcript_data):
        """
        Process the transcript to generate optimal chunks using sentence-based method.

        Args:
            transcript_data: Transcript data from ASR script

        Returns:
            List of optimized chunks
        """
        print(f"Chunking transcript using sentence boundaries with max_tokens={self.max_tokens}")

        # Generate initial chunks based on sentences
        # chunks = self.chunk_by_tokens(transcript_data) # Old method
        chunks = self.chunk_by_sentences(transcript_data)
        print(f"Initial sentence chunking created {len(chunks)} chunks")

        # Filter out chunks that are too short (instead of merging)
        # optimized_chunks = self.optimize_chunks(chunks) # Old method
        filtered_chunks = self.filter_short_chunks(chunks)
        print(f"After filtering short chunks: {len(filtered_chunks)} chunks remain")

        # Analyze semantic quality of the remaining chunks
        final_chunks = self.analyze_chunks_semantic_quality(filtered_chunks)

        return final_chunks

def main():
    parser = argparse.ArgumentParser(description="Chunk ASR transcript into semantic segments based on sentences")
    parser.add_argument("--input", required=True, help="Path to input JSON transcript file")
    parser.add_argument("--output", required=True, help="Path to output CSV file for chunks")
    parser.add_argument("--max_tokens", type=int, default=200,
                        help="Maximum NLTK tokens per chunk (default: 200)")
    parser.add_argument("--overlap", type=int, default=30, # Increased default overlap slightly for sentence method
                        help="Target overlap tokens between chunks (default: 30)")
    parser.add_argument("--min_chunk_tokens", type=int, default=10, # Default min tokens to keep a chunk
                        help="Minimum NLTK tokens for a chunk to be kept (default: 10)")

    args = parser.parse_args()

    # Verify input file exists
    if not os.path.exists(args.input):
        print(f"Error: Input file {args.input} does not exist")
        return

    # Load transcript data
    try:
        with open(args.input, 'r', encoding='utf-8') as f:
            transcript_data = json.load(f)
    except json.JSONDecodeError as e:
        print(f"Error reading JSON file {args.input}: {e}")
        return
    except Exception as e:
        print(f"An unexpected error occurred loading {args.input}: {e}")
        return

    # Validate basic structure needed from transcript_data
    if "transcript" not in transcript_data or "text" not in transcript_data["transcript"] or "segments" not in transcript_data["transcript"] or "audio_id" not in transcript_data:
         print(f"Error: Input JSON {args.input} is missing required keys ('audio_id', 'transcript', 'transcript.text', 'transcript.segments').")
         return


    # Initialize chunker
    try:
        chunker = TranscriptChunker(
            max_tokens=args.max_tokens,
            overlap_tokens=args.overlap,
            min_chunk_tokens=args.min_chunk_tokens
        )
    except ValueError as e:
         print(f"Error initializing chunker: {e}")
         return
    except Exception as e: # Catch potential spaCy loading errors here too
         print(f"An unexpected error occurred during chunker initialization: {e}")
         return


    # Process transcript
    try:
        chunks = chunker.process_transcript(transcript_data)
    except Exception as e:
        print(f"An error occurred during transcript processing: {e}")
        # Potentially log traceback here for debugging
        import traceback
        traceback.print_exc()
        return


    # Create DataFrame
    if chunks:
        chunks_df = pd.DataFrame(chunks)
        # Ensure column order matches expectations if necessary (though CSV doesn't strictly enforce order)
        # expected_cols = ["clip_id", "audio_id", "start_time", "end_time", "transcript", "created_at", "updated_at", "confidence_score"]
        # chunks_df = chunks_df[expected_cols]

        # Save to CSV
        try:
            chunks_df.to_csv(args.output, index=False, encoding='utf-8')
            print(f"Saved {len(chunks)} chunks to: {args.output}")
        except Exception as e:
            print(f"Error saving chunks to CSV {args.output}: {e}")
    else:
        print("No chunks were generated or kept after filtering.")
        # Create an empty CSV with headers to avoid breaking downstream processes expecting the file
        try:
             pd.DataFrame(columns=["clip_id", "audio_id", "start_time", "end_time", "transcript", "created_at", "updated_at", "confidence_score"]).to_csv(args.output, index=False, encoding='utf-8')
             print(f"Saved empty CSV with headers to: {args.output}")
        except Exception as e:
             print(f"Error saving empty CSV to {args.output}: {e}")


if __name__ == "__main__":
    main()

    # python chunk_script.py --input transcript.json --output chunks.csv --max_tokens 200 --overlap 20 --min_chunk_tokens 10